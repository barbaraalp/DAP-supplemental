{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration\n",
    "\n",
    "This Notebook integrates different sources of input to create a feature vector for a region. Here we employ the following information:\n",
    "\n",
    "* Traffic\n",
    "* Weather\n",
    "* Points of Interest \n",
    "\n",
    "These feature categories are time-variant, and describe a 15 minutes time interval for a geographical region R of size 5km x 5km. The final vector also employs time-related features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import psutil\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from multiprocessing import cpu_count,Pool \n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Refine POI Vector for each Geohash (or geographical region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "geohash_map = pd.read_csv(\"geohash_to_poi_vec.csv\")\n",
    "geohash_vec = geohash_map[[ u'Amenity', u'Bump', u'Crossing', u'Give_Way',\n",
    "       u'Junction', u'Noexit', u'Railway', u'Roundabout', u'Station', u'Stop',\n",
    "       u'Traffic_Calming', u'Traffic_Signal', u'Turning_Circle',\n",
    "       u'Turning_Loop']]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(geohash_vec.loc[:,'Amenity':]) \n",
    "scaled_values = scaler.transform(geohash_vec.loc[:,'Amenity':]) \n",
    "geohash_vec.loc[:,'Amenity':] = scaled_values\n",
    "\n",
    "geohash_dict={}\n",
    "for index, row in geohash_map.iterrows():\n",
    "    geohash_dict[row.Geohash] = np.array(geohash_vec.iloc[index])\n",
    "    \n",
    "f = open(\"geo_vect_dict.pkl\",\"wb\")\n",
    "pickle.dump(geohash_dict,f)\n",
    "f.close()\n",
    "\n",
    "geo_dict = dict(zip(geohash_map.Geohash.unique(), range(len(geohash_map.Geohash.unique()))))\n",
    "f = open(\"geo_dict.pkl\",\"wb\")\n",
    "pickle.dump(geo_dict,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Refine Description2Vector data for each Geohash (or geographical region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_map = pd.read_csv(\"geohash_to_text_vec.csv\")\n",
    "\n",
    "NLP_dict={}\n",
    "for index, row in geohash_map.iterrows():\n",
    "    NLP_dict[row.Geohash] = np.array([float(x) for x in row.vec.split(' ')])\n",
    "\n",
    "f = open(\"NLP_vect_dict.pkl\",\"wb\")\n",
    "pickle.dump(NLP_dict,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Data Clearning Steps to make Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filepath,storename):\n",
    "    df = pd.read_csv(filepath)\n",
    "    display (df.head())\n",
    "    \n",
    "    list_ = df.columns\n",
    "    print (list_)\n",
    "    \n",
    "    temp_df = df [[u'TimeStep', u'T-Accident',u'Geohash', u'HOD', u'DOW', u'DayLight',\n",
    "       u'T-BrokenVehicle', u'T-Congestion', u'T-Construction', u'T-Event',\n",
    "       u'T-FlowIncident', u'T-Other', u'T-RoadBlocked', u'W-Humidity',\n",
    "       u'W-Precipitation', u'W-Pressure', u'W-Temperature', u'W-Visibility',\n",
    "       u'W-WindSpeed', u'W-Rain', u'W-Snow', u'W-Fog', u'W-Hail']]\n",
    "    temp_df.to_hdf(storename+'.h5',key='set1')\n",
    "    display(temp_df.head())\n",
    "    \n",
    "    print (\"zero accident =\",float(df[df['T-Accident']==0].shape[0])/df.shape[0])\n",
    "    \n",
    "    f = open(\"geo_dict.pkl\",\"rb\")\n",
    "    geo_dict = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    def fun_hash(geohash):\n",
    "        return geo_dict[geohash]\n",
    "    df['geohash_code'] = df.apply(lambda row: fun_hash(row['Geohash']), axis=1) \n",
    "    temp_df = df [[u'TimeStep', u'T-Accident',u'Geohash',u'geohash_code', u'HOD', u'DOW', u'DayLight',\n",
    "       u'T-BrokenVehicle', u'T-Congestion', u'T-Construction', u'T-Event',\n",
    "       u'T-FlowIncident', u'T-Other', u'T-RoadBlocked', u'W-Humidity',\n",
    "       u'W-Precipitation', u'W-Pressure', u'W-Temperature', u'W-Visibility',\n",
    "       u'W-WindSpeed', u'W-Rain', u'W-Snow', u'W-Fog', u'W-Hail']]\n",
    "    temp_df.to_hdf(storename+'.h5',key='set2')\n",
    "    \n",
    "    df = pd.read_hdf(storename+'.h5',key='set2')\n",
    "    display(df.head())\n",
    "    \n",
    "    def week_day(DOW):\n",
    "        if DOW < 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def shift(group):\n",
    "        df_list=[]\n",
    "        for idx,df in group:\n",
    "            df['predicted_accident'] = df['T-Accident'].shift(-1)\n",
    "            df.drop(df.tail(1).index,inplace=True)\n",
    "            df_list.append(df)\n",
    "        return pd.concat(df_list)\n",
    "\n",
    "    def time_interval(HOD):\n",
    "        if HOD >=6 and HOD <10:\n",
    "            return 0\n",
    "        if HOD >= 10 and HOD<15:\n",
    "            return 1\n",
    "        if HOD >=15 and HOD< 18:\n",
    "            return 2;\n",
    "        if HOD >=18 and HOD< 22:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4; \n",
    "    def make_binary(d):\n",
    "        if d > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0    \n",
    "    df['DOW_cat'] = df.apply(lambda row: week_day(row['DOW']), axis=1)   \n",
    "    df['HOD_cat'] = df.apply(lambda row: time_interval(row['HOD']), axis=1) \n",
    "    df['T-Accident'] = df.apply(lambda row: make_binary(row['T-Accident']), axis=1) \n",
    "    group = df.groupby('Geohash')\n",
    "    df = shift(group)\n",
    "    temp_df = df [[u'TimeStep', u'predicted_accident',u'Geohash',u'geohash_code', u'HOD_cat', u'DOW_cat', u'T-Accident',u'DayLight',\n",
    "       u'T-BrokenVehicle', u'T-Congestion', u'T-Construction', u'T-Event',\n",
    "       u'T-FlowIncident', u'T-Other', u'T-RoadBlocked', u'W-Humidity',\n",
    "       u'W-Precipitation', u'W-Pressure', u'W-Temperature', u'W-Visibility',\n",
    "       u'W-WindSpeed', u'W-Rain', u'W-Snow', u'W-Fog', u'W-Hail']]\n",
    "    temp_df.to_hdf(storename+'.h5',key='set3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cities = ['Atlanta', 'Austin', 'Charlotte', 'Dallas', 'Houston', 'LosAngeles']\n",
    "\n",
    "for city in cities:\n",
    "    clean_data(\"{}_geo2vec_201861-2018831.csv\".format(city), city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_app=[]\n",
    "for city in cities:\n",
    "    list_app.append(pd.read_hdf(city+'.h5',key='set3'))\n",
    "list_app\n",
    "final_df = pd.concat(list_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_hdf('all_cities'+'.h5',key='set3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 (Conda 5.2) [python/2.7 ]",
   "language": "python",
   "name": "sys_python27conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
