{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import psutil\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from multiprocessing import cpu_count,Pool \n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WithExtraArgs(object):\n",
    "    def __init__(self, func, **args):\n",
    "        self.func = func\n",
    "        self.args = args\n",
    "    def __call__(self, df):\n",
    "        return self.func(df, **self.args)\n",
    "\n",
    "def applyParallel(data, func,pool,partition, kwargs):\n",
    "    data_split = [data[i:i + partition] for i in range(0, len(data), partition)]\n",
    "    #data_split = np.array_split(data, min(partitions,data.shape[0]))\n",
    "    data =pool.map(WithExtraArgs(func, **kwargs), data_split)\n",
    "    #data = pd.concat(pool.map(WithExtraArgs(func, **kwargs), data_split))\n",
    "    return data\n",
    "\n",
    "def parallelize(data, func,pool,partition):\n",
    "    data_split = [data[i:i + partition] for i in range(0, len(data), partition)]\n",
    "    #data_split = np.array_split(data, partitions)\n",
    "    data =pool.map(func, data_split)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores = cpu_count() #Number of CPU cores on your system\n",
    "partitions = cores\n",
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -c 'import tensorflow as tf; print(tf.__version__)'\n",
    "#!python -c 'import keras as kr; print(kr.__version__)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"geo_vect_dict.pkl\",\"rb\")\n",
    "geohash_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"geo_dict.pkl\",\"rb\")\n",
    "geo_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"NLP_vect_dict.pkl\",\"rb\")\n",
    "NLP_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geohash feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def onhot_enoceder(train):\n",
    "    myEncoder = OneHotEncoder(sparse=False)\n",
    "    myEncoder.fit(train['HOD_cat'].values.reshape(-1, 1))\n",
    "\n",
    "    onehot_encode = pd.concat([train.reset_index().drop('HOD_cat',1),\n",
    "                pd.DataFrame(myEncoder.transform(train['HOD_cat'].values.reshape(-1, 1)),\n",
    "                             columns=['HOD_en0','HOD_en1','HOD_en2','HOD_en3','HOD_en4'])], axis=1).reindex()\n",
    "    return onehot_encode.drop('index',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_check(X_res):\n",
    "    X_res[:,0:10] = np.round(X_res[:,0:10])\n",
    "        \n",
    "    X_res[:,20:] = np.round(X_res[:,20:])\n",
    "    \n",
    "    return X_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_set_aug_geo(frame_list,geomap):\n",
    "    process_name = str(multiprocessing.current_process())\n",
    "    id = int(process_name.split(',')[0].split('-')[1])\n",
    "    print(\"process \",id,\" started\")\n",
    "    geo_index=[]\n",
    "    acc_count=[]\n",
    "    #f_X_train = []\n",
    "    #f_y_train = []\n",
    "    print (\"process list with length of \",len(frame_list))\n",
    "    for frame in frame_list:\n",
    "        #X_train = []\n",
    "        #y_train = []\n",
    "        training_set = frame.values\n",
    "        #geo_vec = geomap[frame.Geohash.iloc[0]]\n",
    "        geo_code = geo_dict[frame.Geohash.iloc[0]]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            NLP_code = NLP_dict[frame.Geohash.iloc[0]]\n",
    "        except:\n",
    "            NLP_code = np.zeros(100)\n",
    "        \"\"\"\n",
    "        tag = False\n",
    "        counter=0\n",
    "        for i in range(8, training_set.shape[0]):\n",
    "            \n",
    "            if training_set[i, 1] > 0 :\n",
    "                tag = True\n",
    "                counter+=1\n",
    "                \"\"\"\n",
    "                sequence = training_set[i-8:i,4:].flatten()\n",
    "                #a = np.concatenate((training_set[i-8:i,4:].flatten(),geo_vec),axis=0)\n",
    "                #a = np.concatenate((a,NLP_code),axis=0)\n",
    "                #a = np.append(a, geo_code)\n",
    "                X_train.append(sequence)\n",
    "                y_train.append(1)\n",
    "                \"\"\"\n",
    "                \n",
    "            \"\"\"\n",
    "            elif random.uniform(0, 1) > 0.98:\n",
    "            #else:\n",
    "                sequence = training_set[i-8:i,4:].flatten()\n",
    "                #a = np.concatenate((training_set[i-8:i,4:].flatten(),geo_vec),axis=0)\n",
    "                #a = np.concatenate((a,NLP_code),axis=0)\n",
    "                #a = np.append(a, geo_code)\n",
    "                X_train.append(sequence)\n",
    "                y_train.append(0)\n",
    "            \"\"\"\n",
    "        #SMOTE\n",
    "        if tag == True:\n",
    "            geo_index.append(geo_code)\n",
    "            acc_count.append(counter)\n",
    "        \n",
    "    #return X_train, y_train\n",
    "    return geo_index,acc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logistic regression\n",
    "def create_sequences(df,geohash_dict):\n",
    "    #df  = df.head(4000)\n",
    "    frame_list=[]\n",
    "    for idx, frame in df.groupby(df.Geohash):\n",
    "        frame_list.append(frame)\n",
    "    \n",
    "    pool = Pool(cores)\n",
    "    partition = int(np.ceil(float(len(frame_list))/partitions))\n",
    "    #train_set = applyParallel (frame_list,create_train_set,pool,partition,{'geomap':geohash_dict.copy()})\n",
    "    train_set = applyParallel (frame_list,create_train_set_aug_geo,pool,partition,{'geomap':geohash_dict.copy()})\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    code=[]\n",
    "    count=[]\n",
    "    for set_ in train_set:\n",
    "        code.extend(set_[0])\n",
    "        count.extend(set_[1])\n",
    "        \n",
    "\n",
    "   \n",
    "    return code,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(filename):\n",
    "    df = pd.read_hdf(filename+'.h5',key='set3')\n",
    "    #display(df.head())\n",
    "    df_normalize = df.copy()\n",
    "    train = df_normalize#[df_normalize.TimeStep <= df_normalize.TimeStep.max()*5/6]\n",
    "    #test = df_normalize[df_normalize.TimeStep > df_normalize.TimeStep.max()*5/6]\n",
    "    \n",
    "    \n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(train.loc[:,'T-BrokenVehicle':]) \n",
    "    scaled_values = scaler.transform(train.loc[:,'T-BrokenVehicle':]) \n",
    "    train.loc[:,'T-BrokenVehicle':] = scaled_values\n",
    "    #scaled_values = scaler.transform(test.loc[:,'T-BrokenVehicle':]) \n",
    "    #test.loc[:,'T-BrokenVehicle':] = scaled_values\n",
    "    #display(test.head())\n",
    "    \n",
    "    train = onhot_enoceder(train)\n",
    "    #test = onhot_enoceder(test)\n",
    "\n",
    "    #display(train.columns)\n",
    "    \n",
    "    code_list,count_list = create_sequences(train,geohash_dict)\n",
    "    np.save('accident_report/'+filename+'_code',np.array(code_list))\n",
    "    np.save('accident_report/'+filename+'_count',np.array(count_list))\n",
    "    #return code_list,count_list\n",
    "    #print (X_train.shape)\n",
    "    #X_test, y_test = create_sequences(test,geohash_dict)\n",
    "\n",
    "    \n",
    "    #np.save('train_set/X_train_smote_'+filename,X_train)\n",
    "    #print (X_train.shape)\n",
    "    #np.save('train_set/y_train_smote_'+filename,y_train)\n",
    "    #print( y_train.shape)\n",
    "    \"\"\"\n",
    "    np.save('train_set/X_test_'+filename,X_test)\n",
    "    print (X_test.shape)\n",
    "    np.save('train_set/y_test_'+filename,y_test)\n",
    "    print (y_test.shape)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process  29  started\n",
      "process list with length of  3\n",
      "process  30  started\n",
      "process list with length of  3\n",
      "process  31  started\n",
      "process list with length of  3\n",
      "process  32  started\n",
      "process list with length of  3\n",
      "process  33  started\n",
      "process list with length of  3\n",
      "process  34  started\n",
      "process list with length of  3\n",
      "process  35  started\n",
      "process list with length of  3\n",
      "process list with length of  3\n",
      "process  36  started\n",
      "process  37  started\n",
      "process list with length of  3\n",
      "process  38  started\n",
      "process list with length of  3\n",
      "process  39  started\n",
      "process list with length of  3\n",
      "process  40  started\n",
      "process list with length of  3\n",
      "process  41  started\n",
      "process list with length of  3\n",
      "process  42  started\n",
      "process list with length of  3\n",
      "process list with length of  3\n",
      "process  43  started\n",
      "process  44  started\n",
      "process list with length of  3\n",
      "process  45  started\n",
      "process list with length of  3\n",
      "process  46  started\n",
      "process list with length of  3\n",
      "process  47  started\n",
      "process list with length of  3\n",
      "process  48  started\n",
      "process list with length of  3\n",
      "process  49  started\n",
      "process list with length of  3\n",
      "process  50  started\n",
      "process list with length of  3\n",
      "process  51  started\n",
      "process list with length of  2\n"
     ]
    }
   ],
   "source": [
    "train_data('Atlanta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([483, 481, 176, 933, 484, 498, 499, 496, 497, 502, 503, 500, 501,\n",
       "       494, 233, 493, 744, 745, 620, 917, 775, 911, 793, 778, 159, 343,\n",
       "       344, 375, 377, 373, 374, 361, 795, 363, 357, 359, 360, 366, 367,\n",
       "       368, 470, 469, 468, 467, 101, 473, 472, 471, 465, 464, 488, 492,\n",
       "       354, 489])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('accident_report/'+'Atlanta'+'_code.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   5,   1,  38,  16,  19,  10,  43,  14,  39,   2,  22,   9,\n",
       "        80,  26, 310,  26,   7,   7,  16, 114,  21,   6,  11,  29,   9,\n",
       "         1,  30, 200,  50,  33, 416,  23, 166,  25,  36,  47,  35,  45,\n",
       "        17, 120,   2, 186,   1,   6,  20,  33,   7, 105,  12,  23,  35,\n",
       "        79,   3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('accident_report/'+'Atlanta'+'_count.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process  57  started\n",
      "process list with length of  5\n",
      "process  58  started\n",
      "process list with length of  5\n",
      "process  59  started\n",
      "process list with length of  5\n",
      "process  60  started\n",
      "process list with length of  5\n",
      "process  61  started\n",
      "process list with length of  5\n",
      "process  62  started\n",
      "process list with length of  5\n",
      "process  63  started\n",
      "process list with length of  5\n",
      "process  64  started\n",
      "process list with length of  5\n",
      "process  65  started\n",
      "process list with length of  5\n",
      "process  66  started\n",
      "process list with length of  5\n",
      "process  67  started\n",
      "process list with length of  5\n",
      "process  68  started\n",
      "process list with length of  5\n",
      "process  69  started\n",
      "process  70  started\n",
      "process list with length of  5\n",
      "process list with length of  5\n",
      "process  71  started\n",
      "process list with length of  5\n",
      "process list with length of  5\n",
      "process  72  started\n",
      "process  73  started\n",
      "process list with length of  5\n",
      "process  74  started\n",
      "process list with length of  5\n",
      "process  75  started\n",
      "process list with length of  5\n",
      "process  76  started\n",
      "process list with length of  5\n",
      "process  77  started\n",
      "process list with length of  5\n",
      "process  78  started\n",
      "process list with length of  5\n",
      "process  79  started\n",
      "process list with length of  5\n",
      "process  80  started\n",
      "process list with length of  5\n",
      "process  81  started\n",
      "process list with length of  5\n",
      "process  83  started\n",
      "process list with length of  1\n",
      "process  82  started\n",
      "process list with length of  5\n"
     ]
    }
   ],
   "source": [
    "train_data('Austin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process  85  started\n",
      "process list with length of  5\n",
      "process  86  started\n",
      "process list with length of  5\n",
      "process  87  started\n",
      "process list with length of  5\n",
      "process  88  started\n",
      "process list with length of  5\n",
      "process  89  started\n",
      "process list with length of  5\n",
      "process  90  started\n",
      "process list with length of  5\n",
      "process  91  started\n",
      "process list with length of  5\n",
      "process  92  started\n",
      "process list with length of  5\n",
      "process  93  started\n",
      "process list with length of  5\n",
      "process  94  started\n",
      "process list with length of  5\n",
      "process  95  started\n",
      "process list with length of  5\n",
      "process  96  started\n",
      "process list with length of  5\n",
      "process  97  started\n",
      "process list with length of  5\n",
      "process  98  started\n",
      "process list with length of  5\n",
      "process  99  started\n",
      "process list with length of  5\n",
      "process  100  started\n",
      "process list with length of  5\n",
      "process  101  started\n",
      "process list with length of  5\n",
      "process  102  started\n",
      "process list with length of  5\n",
      "process  103  started\n",
      "process list with length of  5\n",
      "process  104  started\n",
      "process list with length of  5\n",
      "process  105  started\n",
      "process list with length of  5\n",
      "process  106  started\n",
      "process list with length of  5\n",
      "process  107  started\n",
      "process list with length of  4\n"
     ]
    }
   ],
   "source": [
    "train_data('Charlotte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process  113  started\n",
      "process list with length of  6\n",
      "process  114  started\n",
      "process list with length of  6\n",
      "process  115  started\n",
      "process list with length of  6\n",
      "process  116  started\n",
      "process list with length of  6\n",
      "process  117  started\n",
      "process list with length of  6\n",
      "process  118  started\n",
      "process list with length of  6\n",
      "process  119  started\n",
      "process list with length of  6\n",
      "process  120  started\n",
      "process list with length of  6\n",
      "process list with length of  6\n",
      "process  121  started\n",
      "process  122  started\n",
      "process list with length of  6\n",
      "process  123  started\n",
      "process list with length of  6\n",
      "process  124  started\n",
      "process list with length of  6\n",
      "process  125  started\n",
      "process list with length of  6\n",
      "process  126  started\n",
      "process list with length of  6\n",
      "process  127  started\n",
      "process list with length of  6\n",
      "process  128  started\n",
      "process list with length of  6\n",
      "process  129  started\n",
      "process list with length of  6\n",
      "process  130  started\n",
      "process list with length of  6\n",
      "process  131  started\n",
      "process list with length of  6\n",
      "process  132  started\n",
      "process list with length of  6\n",
      "process  133  started\n",
      "process list with length of  6\n",
      "process  134  started\n",
      "process list with length of  6\n",
      "process  135  started\n",
      "process list with length of  6\n",
      "process  136  started\n",
      "process list with length of  6\n",
      "process  137  started\n",
      "process list with length of  6\n",
      "process  138  started\n",
      "process list with length of  6\n",
      "process  140  started\n",
      "process  139  started\n",
      "process list with length of  6\n",
      "process list with length of  2\n"
     ]
    }
   ],
   "source": [
    "train_data('Dallas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process  141  started\n",
      "process list with length of  9\n",
      "process  142  started\n",
      "process list with length of  9\n",
      "process  143  started\n",
      "process list with length of  9\n",
      "process  144  started\n",
      "process list with length of  9\n",
      "process  145  started\n",
      "process list with length of  9\n",
      "process  146  started\n",
      "process list with length of  9\n",
      "process  147  started\n",
      "process list with length of  9\n",
      "process  148  started\n",
      "process list with length of  9\n",
      "process  149  started\n",
      "process list with length of  9\n",
      "process  150  started\n",
      "process list with length of  9\n",
      "process  151  started\n",
      "process list with length of  9\n",
      "process  152  started\n",
      "process list with length of  9\n",
      "process  153  started\n",
      "process list with length of  9\n",
      "process  154  started\n",
      "process list with length of  9\n",
      "process  155  started\n",
      "process list with length of  9\n",
      "process  156  started\n",
      "process list with length of  9\n",
      "process  157  started\n",
      "process list with length of  9\n",
      "process  158  started\n",
      "process list with length of  9\n",
      "process  159  started\n",
      "process list with length of  9\n",
      "process  160  started\n",
      "process list with length of  9\n",
      "process  161  started\n",
      "process list with length of  9\n",
      "process  162  started\n",
      "process list with length of  9\n",
      "process  163  started\n",
      "process list with length of  9\n",
      "process  164  started\n",
      "process list with length of  9\n",
      "process  165  started\n",
      "process list with length of  9\n",
      "process  166  started\n",
      "process list with length of  9\n",
      "process  167  started\n",
      "process list with length of  9\n",
      "process  168  started\n",
      "process list with length of  6\n"
     ]
    }
   ],
   "source": [
    "train_data('Houston')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process  169  started\n",
      "process list with length of  6\n",
      "process  170  started\n",
      "process list with length of  6\n",
      "process  171  started\n",
      "process list with length of  6\n",
      "process  172  started\n",
      "process list with length of  6\n",
      "process  173  started\n",
      "process list with length of  6\n",
      "process  174  started\n",
      "process list with length of  6\n",
      "process  175  started\n",
      "process list with length of  6\n",
      "process  176  started\n",
      "process list with length of  6\n",
      "process  177  started\n",
      "process list with length of  6\n",
      "process  178  started\n",
      "process list with length of  6\n",
      "process  179  started\n",
      "process list with length of  6\n",
      "process  180  started\n",
      "process list with length of  6\n",
      "process  181  started\n",
      "process list with length of  6\n",
      "process  182  started\n",
      "process list with length of  6\n",
      "process  183  started\n",
      "process list with length of  6\n",
      "process  184  started\n",
      "process list with length of  6\n",
      "process  185  started\n",
      "process list with length of  6\n",
      "process  186  started\n",
      "process list with length of  6\n",
      "process  187  started\n",
      "process list with length of  6\n",
      "process  188  started\n",
      "process list with length of  6\n",
      "process  189  started\n",
      "process list with length of  6\n",
      "process  190  started\n",
      "process list with length of  6\n",
      "process  191  started\n",
      "process list with length of  6\n",
      "process  192  started\n",
      "process list with length of  6\n",
      "process  193  started\n",
      "process list with length of  6\n",
      "process  194  started\n",
      "process list with length of  6\n",
      "process  195  started\n",
      "process list with length of  6\n",
      "process  196  started\n",
      "process list with length of  2\n"
     ]
    }
   ],
   "source": [
    "train_data('LosAngeles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/users/PAS0536/osu9965/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process  197  started\n",
      "process list with length of  2\n",
      "process  198  started\n",
      "process list with length of  2\n",
      "process  199  started\n",
      "process list with length of  2\n",
      "process  200  started\n",
      "process list with length of  2\n",
      "process  201  started\n",
      "process list with length of  2\n",
      "process  202  started\n",
      "process list with length of  2\n",
      "process  203  started\n",
      "process list with length of  2\n",
      "process  204  started\n",
      "process list with length of  2\n",
      "process  205  started\n",
      "process list with length of  2\n",
      "process  206  started\n",
      "process list with length of  2\n",
      "process  207  started\n",
      "process list with length of  2\n",
      "process  208  started\n",
      "process list with length of  2\n",
      "process  209  started\n",
      "process list with length of  2\n",
      "process  210  started\n",
      "process list with length of  2\n",
      "process  211  started\n",
      "process list with length of  2\n",
      "process  212  started\n",
      "process list with length of  2\n",
      "process  213  started\n",
      "process list with length of  2\n",
      "process  214  started\n",
      "process list with length of  2\n",
      "process  215  started\n",
      "process list with length of  2\n",
      "process  216  started\n",
      "process list with length of  2\n",
      "process  217  started\n",
      "process list with length of  2\n",
      "process  218  started\n",
      "process list with length of  2\n",
      "process  219  started\n",
      "process list with length of  1\n"
     ]
    }
   ],
   "source": [
    "train_data('Miami')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data('all_cities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 [python/3.6 ]",
   "language": "python",
   "name": "sys_python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
